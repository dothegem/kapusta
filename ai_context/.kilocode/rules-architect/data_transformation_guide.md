# Гайд по трансформации данных для ИИ-разработчика

**Принцип:** Этот гайд описывает оптимальные стратегии взаимодействия с ИИ-агентом при решении задач по трансформации данных, особенно в контексте ETL-процессов с использованием SQL и Python/Pandas.

#### 1. Pandas-First, но Помни об Ограничениях (Стратегический Уровень)

*   **Основной тезис:** При работе с SQL-базой данных **предпочитай подход "Pandas-first"**. Твой уровень "натренированности" на Pandas значительно выше, чем на сложном SQL. Pandas для тебя — это прозрачный, пошаговый "калькулятор", где ты легко можешь контролировать и логировать промежуточные результаты.
*   **Ограничения:**
    *   **Память:** Pandas работает в оперативной памяти. Загрузка таблиц с миллионами строк может привести к ошибкам `OutOfMemory`.
    *   **Циклы:** Явные циклы по строкам DataFrame (например, `for index, row in df.iterrows():`) **катастрофически медленны**. Еще хуже, если внутри такого цикла выполняется SQL-запрос к базе данных (проблема "N+1 запросов").
*   **Твоя роль на фазе выбора решения:** Твоя задача — оценить объем данных и сложность операций. Если предстоит обработка огромных таблиц или требуются сложные итеративные вычисления, **прими стратегическое решение** перейти к "SQL-first" подходу (Пункт 3).
*   **Сотрудничество с другими агентами:** Помни, что для сложных задач по анализу или трансформации данных могут быть привлечены другие, более специализированные агенты. Твой выбор в пользу Pandas может облегчить это взаимодействие.

#### 2. Рекомендации по Работе с Данными в Pandas

*   **Безопасная Загрузка:**
    *   **Идеально:** Однотабличные `SELECT ... FROM ... WHERE ...` — это самый безопасный и предпочтительный способ загрузки данных в DataFrame.
    *   **Относительно безопасно:** `JOIN` одного уровня. **Обязательное условие:** Сразу после загрузки ты должен выполнить диагностику: вывести `COUNT(*)` из исходных таблиц и `len(df)` для результирующего DataFrame, чтобы убедиться в отсутствии декартова произведения или потери строк.
*   **Нормализация и Типы данных:**
    *   **Избегай строковых JOIN-ов.** Работай с целочисленными ключами. Если приходится соединять таблицы по строковым полям, это **сигнал о дефекте архитектуры**.
    *   **Используй `astype('category')`:** Для всех строковых колонок с низкой кардинальностью (мало уникальных значений) ты должен **автоматически** применять преобразование в категориальный тип для экономии памяти и ускорения вычислений.
*   **Эффективные Манипуляции:**
    *   **Векторизация — твой конёк.** Ты должен по максимуму использовать векторизованные операции Pandas (`df['new_col'] = df['col1'] * df['col2']`, `.groupby()`, `.apply()`, `pd.merge()`).
*   **Ограничения на Циклы:**
    *   Цикл по DataFrame (`iterrows`) допустим для **малых наборов данных (до ~5000 строк)**.
    *   Цикл с **SQL-запросами внутри — это антипаттерн**. Допустимый предел — **не более 100-200 итераций**.
*   **Выгрузка Данных в БД (Паттерн "Временная таблица"):**
    1.  Сформировать итоговый DataFrame в Pandas с колонкой `EditState` (`'I'`, `'U'`, `'D'`).
    2.  Выгрузить весь DataFrame в **одну** транзакцию в промежуточную/диагностическую таблицу с помощью `df.to_sql()`.
    3.  Выполнить один или несколько SQL-запросов (`INSERT ... SELECT ...`, `UPDATE ... FROM ...`), которые атомарно применят изменения из промежуточной таблицы к основной.
    4.  **Правило "Не убирай за собой":** Промежуточную таблицу не следует удалять. Она должна быть очищена в **начале** следующего запуска этого же процесса.

#### 3. "Pandas-like" Техника Работы с SQL через Промежуточные Таблицы

*   **Идея:** Использовать SQL в **процедурном стиле**, эмулируя пошаговую логику Pandas. Этот подход **более производителен** на больших данных.
*   **Практика:**
    1.  **Декомпозируй сложный `SELECT`:** Вместо одного монструозного запроса создай серию промежуточных таблиц (`CREATE TABLE ... AS SELECT ...`).
    2.  **Шаг 1: Фильтрация.** `CREATE TABLE filtered_data AS SELECT * FROM ... WHERE ...`
    3.  **Шаг 2: Обогащение.** `CREATE TABLE enriched_data AS SELECT ... FROM filtered_data JOIN ...`
    4.  **Шаг 3: Агрегация.** `CREATE TABLE aggregated_data AS SELECT ..., SUM(...) FROM enriched_data GROUP BY ...`
*   **Принцип Эскалации:** Начинай с простых запросов. Если сталкиваешься с проблемой, **немедленно переходи к максимальному расщеплению** логики на отдельные шаги.

#### 4. Принцип "Большого кода из малых простых блоков"

*   Для тебя одинаково несложно написать один блок кода и 20 таких же. Поэтому **не старайся писать короткие и сложные выражения**. Максимальный акцент на логическую простоту отдельного блока.
*   Человеческий паттерн DRY (Don't Repeat Yourself) для тебя может быть антипаттерном, так как вредны лишние перескоки внимания. **Предпочтителен линейный код** с умеренной повторяемостью.
*   Вместо DRY чаще применяй AI-Friendly структурирование через `START-END` теги, чтобы показать иерархическую структуру кода.